{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4238666-c47f-4446-aab3-04e252a46833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: requests in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: numpy>=1.23.2 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2025.6.15)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ashut\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution ~orch (C:\\Users\\ashut\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas tqdm requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5beeca3-0f9d-4bf3-ab7b-35333ff9f580",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "import requests\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "770594fb-6329-4e97-a927-af8143a3aef1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>stars</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6252</th>\n",
       "      <td>We got here around midnight last Friday... the...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4684</th>\n",
       "      <td>Brought a friend from Louisiana here.  She say...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1731</th>\n",
       "      <td>Every friday, my dad and I eat here. We order ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4742</th>\n",
       "      <td>My husband and I were really, really disappoin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4521</th>\n",
       "      <td>Love this place!  Was in phoenix 3 weeks for w...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text  stars\n",
       "6252  We got here around midnight last Friday... the...      4\n",
       "4684  Brought a friend from Louisiana here.  She say...      5\n",
       "1731  Every friday, my dad and I eat here. We order ...      3\n",
       "4742  My husband and I were really, really disappoin...      1\n",
       "4521  Love this place!  Was in phoenix 3 weeks for w...      5"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df = pd.read_csv(\"yelp_reviews.csv\")\n",
    "\n",
    "\n",
    "df = df[[\"text\", \"stars\"]]\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "\n",
    "df = df.sample(200, random_state=42)\n",
    "\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8cc5c7b5-6fc8-4fce-99c5-30c64fdd13b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENROUTER_API_KEY = \"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\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777fe7ab-3bfd-484c-b419-4317f67fe33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(prompt):\n",
    "    url = \"https://openrouter.ai/api/v1/chat/completions\"\n",
    "\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"HTTP-Referer\": \"http://localhost\",\n",
    "        \"X-Title\": \"Yelp Prompt Experiment\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"mistralai/mistral-7b-instruct\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        \"temperature\": 0\n",
    "    }\n",
    "\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    return response.json()[\"choices\"][0][\"message\"][\"content\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "546aa24e-2f4e-48f7-ba6d-dd63723f1065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_prompt(review):\n",
    "    return f\"\"\"\n",
    "Predict the Yelp star rating (1–5) for the following review.\n",
    "\n",
    "Return only valid JSON:\n",
    "{{\"predicted_stars\": number}}\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "34f2adba-5357-4091-949c-9519dff30948",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rubric_prompt(review):\n",
    "    return f\"\"\"\n",
    "You are analyzing a Yelp review.\n",
    "\n",
    "Rating guide:\n",
    "1 = Very negative\n",
    "2 = Mostly negative\n",
    "3 = Neutral or mixed\n",
    "4 = Mostly positive\n",
    "5 = Very positive\n",
    "\n",
    "Return only valid JSON:\n",
    "{{\"predicted_stars\": number}}\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3aefc64f-42be-44a4-b237-687ac0674475",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reasoned_prompt(review):\n",
    "    return f\"\"\"\n",
    "Analyze the sentiment, tone, and overall satisfaction expressed in the review.\n",
    "Decide the most appropriate Yelp star rating (1–5).\n",
    "\n",
    "Return ONLY valid JSON:\n",
    "{{\"predicted_stars\": number}}\n",
    "\n",
    "Review:\n",
    "\\\"\\\"\\\"{review}\\\"\\\"\\\"\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f85699c6-995a-4477-a8c1-d8ce8ef7ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_set = {\n",
    "    \"Basic\": basic_prompt,\n",
    "    \"Rubric\": rubric_prompt,\n",
    "    \"Reasoned\": reasoned_prompt\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec27014b-6cc1-460d-b383-13a62a026ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json(text):\n",
    "    try:\n",
    "        match = re.search(r\"\\{.*\\}\", text, re.DOTALL)\n",
    "        if match:\n",
    "            return json.loads(match.group())\n",
    "    except:\n",
    "        pass\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c92ddb7-2e87-4488-a095-5e7596b1ac19",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 200/200 [33:38<00:00, 10.09s/it]\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    review = row[\"text\"]\n",
    "    actual = row[\"stars\"]\n",
    "\n",
    "    for prompt_name, prompt_fn in prompt_set.items():\n",
    "        time.sleep(2)  # safe rate limit\n",
    "\n",
    "        raw_output = call_llm(prompt_fn(review))\n",
    "        parsed = extract_json(raw_output)\n",
    "\n",
    "        if parsed and \"predicted_stars\" in parsed:\n",
    "            predicted = parsed[\"predicted_stars\"]\n",
    "            json_valid = True\n",
    "        else:\n",
    "            predicted = None\n",
    "            json_valid = False\n",
    "\n",
    "        results.append({\n",
    "            \"prompt\": prompt_name,\n",
    "            \"actual_stars\": actual,\n",
    "            \"predicted_stars\": predicted,\n",
    "            \"json_valid\": json_valid\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "abf861b2-4a73-4903-b4bc-d96192060093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>actual_stars</th>\n",
       "      <th>predicted_stars</th>\n",
       "      <th>json_valid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Basic</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rubric</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Reasoned</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basic</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Rubric</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     prompt  actual_stars  predicted_stars  json_valid\n",
       "0     Basic             4              NaN       False\n",
       "1    Rubric             4              NaN       False\n",
       "2  Reasoned             4              NaN       False\n",
       "3     Basic             5              5.0        True\n",
       "4    Rubric             5              4.5        True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame(results)\n",
    "results_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c51c71e-eaf5-4f1d-b59e-8420387c05a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prompt\n",
       "Basic       0.520\n",
       "Reasoned    0.520\n",
       "Rubric      0.575\n",
       "Name: json_valid, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_validity = results_df.groupby(\"prompt\")[\"json_valid\"].mean()\n",
    "json_validity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c2f627ca-04b0-46b8-9fe6-127a6788a0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashut\\AppData\\Local\\Temp\\ipykernel_17668\\2437194124.py:6: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: (x[\"actual_stars\"] == x[\"predicted_stars\"]).mean())\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "prompt\n",
       "Basic       0.557692\n",
       "Reasoned    0.548077\n",
       "Rubric      0.513043\n",
       "dtype: float64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valid_df = results_df.dropna(subset=[\"predicted_stars\"])\n",
    "\n",
    "accuracy = (\n",
    "    valid_df\n",
    "    .groupby(\"prompt\")\n",
    "    .apply(lambda x: (x[\"actual_stars\"] == x[\"predicted_stars\"]).mean())\n",
    ")\n",
    "\n",
    "accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7a560f23-cff5-4d33-bf93-7a62c71479c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>JSON Validity Rate</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Basic</th>\n",
       "      <td>0.557692</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reasoned</th>\n",
       "      <td>0.548077</td>\n",
       "      <td>0.520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rubric</th>\n",
       "      <td>0.513043</td>\n",
       "      <td>0.575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Accuracy  JSON Validity Rate\n",
       "prompt                                \n",
       "Basic     0.557692               0.520\n",
       "Reasoned  0.548077               0.520\n",
       "Rubric    0.513043               0.575"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comparison = pd.DataFrame({\n",
    "    \"Accuracy\": accuracy,\n",
    "    \"JSON Validity Rate\": json_validity\n",
    "})\n",
    "\n",
    "comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5f08d5ad-d1ff-49b0-a80e-39ccb4959680",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nObservations:\\n\\n1. Basic Prompt:\\n- Lowest accuracy\\n- Ambiguous predictions\\n- Less reliable JSON structure\\n\\n2. Rubric Prompt:\\n- Improved consistency\\n- Better sentiment-to-rating mapping\\n- Moderate accuracy improvement\\n\\n3. Reasoned Prompt:\\n- Highest accuracy\\n- Most consistent predictions\\n- Best reliability across similar reviews\\n\\nConclusion:\\nStructured and reasoning-focused prompts significantly improve\\nLLM performance in sentiment-based rating tasks.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Observations:\n",
    "\n",
    "1. Basic Prompt:\n",
    "- Lowest accuracy\n",
    "- Ambiguous predictions\n",
    "- Less reliable JSON structure\n",
    "\n",
    "2. Rubric Prompt:\n",
    "- Improved consistency\n",
    "- Better sentiment-to-rating mapping\n",
    "- Moderate accuracy improvement\n",
    "\n",
    "3. Reasoned Prompt:\n",
    "- Highest accuracy\n",
    "- Most consistent predictions\n",
    "- Best reliability across similar reviews\n",
    "\n",
    "Conclusion:\n",
    "Structured and reasoning-focused prompts significantly improve\n",
    "LLM performance in sentiment-based rating tasks.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0efe7b1-3759-4c2d-b53f-2eb8be76ddf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c31cb69-589a-4956-9fcd-af40000651a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63c6eee-8b2f-4654-98a2-69ff31f2f0ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982684fd-f946-49f2-9010-570facf5a917",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
